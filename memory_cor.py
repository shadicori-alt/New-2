# memory_core.py โ ุฎููู (ูุง ูุณุชุฎุฏู transformers)
import json, datetime, diskcache as dc

# ุฐุงูุฑุฉ ููููุฉ ุฎูููุฉ (ุญุฌู ูุงูู < 5 ููุฌุง)
cache = dc.Cache("light_memory", size_limit=50 * 1024 * 1024)   # 50 ููุฌุง ูุญุฏ ุฃูุตู

def save_chat(user_id, question, answer):
    key = f"{user_id}_{datetime.datetime.utcnow().isoformat()}"
    cache[key] = {"q": question, "a": answer, "t": str(datetime.datetime.utcnow())}

def get_chat_history(user_id, last=3):
    keys = [k for k in cache if k.startswith(user_id)]
    keys.sort(reverse=True)
    return [cache[k] for k in keys[:last]]

def search_knowledge(query, top_k=1):
    # ุจุญุซ ุจุณูุท ุจุงููููุงุช ุงูููุชุงุญูุฉ (ุฎููู ูุณุฑูุน)
    results = []
    for k in cache:
        if query in cache[k]["q"] or query in cache[k]["a"]:
            results.append(cache[k]["a"])
    return results[0] if results else None

def reply_sci(user_id, question):
    history = get_chat_history(user_id)
    context = "\n".join([f"ุณ: {h['q']}\nุฌ: {h['a']}" for h in history])
    ans = search_knowledge(question)
    if ans:
        return f"ุจูุงุกู ุนูู ุณุงุจูุชู:\n{ans}"
    # ุฅุฐุง ูู ููุฌุฏ โ ูุฑุฌุน ูู GPT (ููุณุจูุงู ูุฑุจูุท)
    import openai
    openai.api_key = __import__("db").get("openai_key")
    res = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": f"ุณุคุงู ูุญุงุณุจู/ุฅุฏุงุฑู: {question}\nุงูุณูุงู: {context}"}]
    )
    output = res["choices"][0]["message"]["content"]
    save_chat(user_id, question, output)
    return output

def reply_book(user_id, question):
    history = get_chat_history(user_id)
    name = question.split()[0] if question else ""
    return f"ุฃููุงู {name} ๐\nุณุฌูุช ุณุคุงูู: {question}\nูุฑุฌุน ูู ุจุงูุชูุงุตูู ูุงูุญุฌุฒ ุฎูุงู ุฏูุงุฆู โ ููุท ุงุถุบุท ุงูุฒุฑ ุฃุฏูุงู."